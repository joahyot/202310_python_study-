# Gradient Descent Algorithm (경사 하강 알고리즘)

: 비용함수(Cost Function)의 비용값을 최소화하는 파라미터 세타 값을 찾는 경사 하강 알고리즘(Gradient descent Algorithm)

# 작동 원리 
1. 세타에 대해 임의의 초기값 즉 시작점을 잡는다.
2. J(error값)값이 최소가 될 때까지 세타값 갱신을 반복하여 최솟값에 도달했을 때 해당하는 세타를 찾아낸다. 

# 초기 값 설정 후 반복적인 갱신을 통해 최종적인 minimum에 도달하는 것이 최종 목표 


# 알고리즘 구성요소 
(1) 미분값(Derivative Term)
: 해당 지점에서의 기울기를 의미함

- 만약 알파가 일반적인 양수값이라면 세타(=w)가 반복적으로 갱신되면서 최종적으로 최솟값에 도달하게 될 것.
- 세타의 갱신은 기울기 하강과 기울기 상승 두가지로 나눔

- 포물선의 대칭축을 기준으로 오른쪽 영역에서 한 지점을 초기값으로 설정했다고 가정.
- 이 경우 오른쪽 영역의 기울기 즉 미분값은 양수일 것. 그렇기 때문에 갱신 시 세타는 값소하여 최솟값 지점으로 가깝게 이동함.

- 포물선의 대칭축을 기준으로 왼쪽 영역에서 한 지점을 초기값으로 설정했다고 가정.
- 왼쪽 영역의 기울기 즉 미분값은 음수일 것. 그렇기 때문에 갱신 시 세타는 증가하여 최솟값 지점으로 가깝게 이동함.

* 경사 하강 알고리즘에서는 초기값을 어디로 잡아도 결국은 최솟값으로 돌아가는 것! 


(2) learing rate(학습 속도)
- Learing rate가 지나치게 작다면 최솟값에 도달하기 위해 굉장히 많은 연산이 요구됨.
- 반대로 Learning rate가 지나치게 크다면 세타가 반대쪽을 오가며 매우 큰 거리를 이동하게 되어 최솟값에서 점점 멀어지게 된다.
- 적절한 Learning rate 설정이 매우 중요

- 속도에만 영향을 주는 것은 아님
- Learning rate 없이 미분값으로만 갱신을 진행하게 되면 최솟값으로 도달하지 않고, 
  제자리에서 진동할 수 있기 때문에 Learning rate를 곱하여 갱신값에 변화를 주며 최솟값에 도달하게 해야함.


(3) theta의 수렴조건 
:경사 하강 알고리즘은 어떤 조건에서 갱신을 멈추게 될까?

함수는 극점에서의 기울기가 0이라는 사실을 우리는 미적분에서 배웠음. 
즉 만약 우리가 최솟값에 도달했다면 해당 지점에서 미분값이 0일 것이고 이 경우 바로 직전의 값과 갱신값 사이의 변화가 없을 것
이 경우가 Cost Function이 최소가 되는 경우이므로 멈추면 됨.


# Batch Gradient Descent Algorithm
: Batch는 Total training set. 즉 갱신을 하는 매 단계마다 전체 Training set을 활용하였기 때문에 이런 이름을 붙이게 됨. 

장점 
- 전체 없데이트가 한번에 이뤄지게 되므로 계산의 횟수가 적음
- 전체 데이터에 대해 미분값을 계산하여 갱신하므로 안정적으로 수렴함

단점
- 한번의 갱신에 전체 training set이 사용되므로 학습이 오래 걸림
- 갱신 직전까지 전체 trainng set의 오차를 가지고 있어야 하므로 상대적으로 많은 메모리가 요구됨
- Local minimum에 빠지면 나오기가 어려움

